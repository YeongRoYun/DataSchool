{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c1e325c",
   "metadata": {},
   "source": [
    "https://huggingface.co/\n",
    "https://www.edwith.org/search/index?categoryId=71\n",
    "Speech and Language Processing\n",
    "밑바닥부터 시작하는 딥러닝\n",
    "https://ratsgo.github.io/speechbook/\n",
    "\n",
    "# Sequential Model\n",
    "- RNN Model\n",
    "  - LSTM\n",
    "\n",
    "- Attention\n",
    "  - Seq2Seq Model\n",
    "  - Transformer\n",
    "  \n",
    "# Audio\n",
    "\n",
    "## Voice\n",
    "\n",
    "### 화자 인식\n",
    "- 발화자가 누구인가?\n",
    "\n",
    "### 음성 인식\n",
    "- 어떤 음성인가?\n",
    "\n",
    "### 음성 합성\n",
    "- TTS\n",
    "- AI 튜터(Digital human)\n",
    "\n",
    "### 노래\n",
    "- 운율감\n",
    "\n",
    "## Sound\n",
    "\n",
    "### 악기\n",
    "- 소리 생성\n",
    "\n",
    "### 일반적 음향\n",
    "- 소리 생성\n",
    "\n",
    "\n",
    "## Music\n",
    "```음악은 노래 + 악기```\n",
    "\n",
    "### 추천\n",
    "스포티파이\n",
    "\n",
    "### 검색\n",
    "\n",
    "### 합성(작곡)\n",
    "https://magenta.tensorflow.org/\n",
    "\n",
    "### Scoring(노래방)\n",
    "https://freddiemeter.withyoutube.com/\n",
    "\n",
    "### 편집\n",
    "보이저엑스"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e81de3",
   "metadata": {},
   "source": [
    "# DL\n",
    "https://playground.tensorflow.org\n",
    "\n",
    "# Paper\n",
    "https://paperswithcode.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68176f81",
   "metadata": {},
   "source": [
    "# NLP & LM \n",
    "```\n",
    "                             RNN      RNN + Attention        FFNN + Attention\n",
    "--------2013-----------------2017-------2018------------------2019-------------\n",
    "       Word2Vec             Seq2Seq   Attention              Transformer(T)\n",
    "                                          +                       (Pre-trained)\n",
    "                                       Seq2Seq                      BERT (T.Encoder)\n",
    "                                 (Encoder + Decoder)                \n",
    "                                                                     GPT (T.Decoder)\n",
    "```    \n",
    "```\n",
    "Easy----------------------------------------------Hard\n",
    "   - Spelling        - Parsing     - Machine Translation\n",
    "   - Keyword                       - Emotion Analysys\n",
    "   - Simliar words                 - \n",
    "```\n",
    "\n",
    "## Machine Translation\n",
    "- The ambiguity of words <-- Syntax-based translation\n",
    "- The semantic is necessary\n",
    "\n",
    "## Emotion Analysys\n",
    "- From sentence, weighted words & context\n",
    "  - love : .8, sad : .2, etc\n",
    "\n",
    "\n",
    "## Text data\n",
    "\n",
    "### Word to Numeric\n",
    "```\n",
    "Rank(Representive level)\n",
    "\n",
    "  0    Scalar\n",
    "       \n",
    "  1    Vector\n",
    "         - one-hot vector : Sparse\n",
    "         - word2vec : Dense\n",
    "       \n",
    "  2    Matrix\n",
    "       \n",
    "  >3   Tensor\n",
    "```\n",
    "\n",
    "### Similarity\n",
    "- Distance\n",
    "- Cosin-Simliarity(used at Attention)\n",
    "\n",
    "### Word2Vec\n",
    "- Gensim\n",
    "\n",
    "### Word-Vector Relation\n",
    "- How to find good vector-representation!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba569138",
   "metadata": {},
   "source": [
    "# Speech Recognition\n",
    "\n",
    "## Summary\n",
    "```\n",
    "Audio          Voice        Speaker Recognation    \n",
    "                                Verification\n",
    "                                Diarization\n",
    "\n",
    "                            Voice Recognation   Speech to Text\n",
    "                                Voice Modeling\n",
    "                                Language Modeling\n",
    "                                    \n",
    "                            Voice Conversion.   Text to Speech\n",
    "  \n",
    "  \n",
    "  \n",
    "               Sound        Song\n",
    "               \n",
    "                            Music\n",
    "                             \n",
    "                            ETC\n",
    "```\n",
    "\n",
    "## Audio\n",
    "\n",
    "### Concept\n",
    "- 공기나 물같은 매질의 진동을 통해 전달되는 종파\n",
    "- 소리가 발생할 조건\n",
    "  - 매질 ex) 공기, 물\n",
    "  - 파형(정현파) - sin/cos wave\n",
    "- 조음 기관\n",
    "  - 성대\n",
    "  - 악기\n",
    "- 감지 기관\n",
    "  - 귀\n",
    "  - 마이크\n",
    "  \n",
    "### Sin/Cos waves\n",
    "```\n",
    "y(t) = A * sin(2*pi*f*t + 위상) \n",
    "    where A := Amplitude, f := 1/T(period), 위상 := 시간축에 따른 평행이동\n",
    "\n",
    "Signal = SUM(y_n(t))\n",
    "    As : 소리의 세기를 결정\n",
    "    fs : f는 높낮이, fs는 음색\n",
    "    위상 : 조합되는 순서 결정\n",
    "```\n",
    "\n",
    "### Physical Characteristics\n",
    "- 세기(Amplitude) : 인지하는 세기는 Amplitude의 log-scale로 증가한다\n",
    "- 높낮이(f) : 진동수에 의해 결정\n",
    "- 위상 : 조합되는 순서 결정(중첩시, 소리의 크기가 변한다) --> Noise Canceling\n",
    "- 음색(fs) : 여러 파형이 조합되 음색을 결정. 푸리에의 이론으로 sin 파의 조합으로 가능\n",
    "- 전파 속도 : 매질에 영향\n",
    "- 위의 성질 중 하나라도 다르면 다른 소리로 인식된다\n",
    "\n",
    "### Analog to Digital\n",
    "- 샘플링 주파수 : 얼마나 신호를 뽑을 것인가 (시간축을 구분)\n",
    "- 16000 Hz := 1/16000sec 마다 sample 선택\n",
    "- 양자화 := 칸 나누기 (16bit = +-2^16으로 Amplitude를 구분)\n",
    "- 정보의 손실이 발생한다\n",
    "- 나이퀴스트 이론 : 샘플링 주파수는 원 신호 진동수(f)의 최소 2배 이상으로 설정해야 한다\n",
    "  - 목소리 주파수의 한계(max 8000Hz) 등으로 엄청 세밀한 간격이 필요친 않다\n",
    "- DL에서 샘플링된 신호를 그대로 사용하는 경우는 거의 없다\n",
    "  - 짧은 시간을 표현하는데 너무 큰 데이터(16000개의 샘플 = 1초)\n",
    " \n",
    "### 푸리에 변환(FT)\n",
    "https://blog.naver.com/PostView.nhn?isHttpsRedirect=true&blogId=guburi&logNo=221680859032&parentCategoryNo=&categoryNo=52&viewDate=&isShowPopularPosts=true&from=search\n",
    "- Time Domain -> Frequency Domain\n",
    "- Convert all waves to sin waves\n",
    "- 시간 정보가 소실된다\n",
    "  - Short-time FT로 극복!\n",
    "- DL의 입력: Spectrogram\n",
    "  - Digital signal -> Short-time FT (a + bj) -> Spectrogram(||a+bj||)\n",
    "   ```\n",
    "   Frequency ---------------------------\n",
    "       |    |                           |\n",
    "       |--------------------------------|\n",
    "       |    |                           |\n",
    "       |--------------------------------|\n",
    "        --------------------------Frame Time\n",
    "      Window_Size\n",
    "   ```\n",
    "  - Sliding-Window를 이용해 FT를 나타낸다\n",
    "  \n",
    "## 음성 인식\n",
    "- 화자 인식 : 화자는 누구인가?\n",
    "- 음성 인식 : 화자의 발성은 무엇인가?\n",
    "- 주요 구성 요소\n",
    "  - 학습 : 음성 DB, 텍스트 Corpus, 음향모델, 어휘/발음사전, 언어모델\n",
    "  - 예측 : 음성 신호(.wav 등), 잡음처리/음성특징추출, 디코더, 텍스트(출력)\n",
    "    - 입력처리: Spectrogram으로 만들기\n",
    "    - 잡음처리: Spech enhancement, Noise Surpression\n",
    "    - 디코더: 음향/언어/어휘발음사전 모델들을 사용해야한다\n",
    "- 어휘발음사전\n",
    "  - 자연어의 최소 단위는 형태소 (국물 -> ㄱ ㅜ ㄱ ㅁ ㅜ ㄹ)\n",
    "  - 발음 형태는 다르다(국물 -> 궁물)\n",
    "  - 발음하는 단어로 바꿔야한다!!\n",
    "  - 신조어\n",
    "  \n",
    "- 음향/언어 모델\n",
    "  - 각 언어별 모델이 따로 존재하다 Multilingual Model이 나오고 있다\n",
    "  \n",
    "-  End-to-End Model\n",
    "```\n",
    ".wave --- some_model --- output (Done!)\n",
    "```\n",
    "\n",
    "### 음성인식 처리 과정\n",
    "```\n",
    "Audio wave -> Feature Representation(Spectrogram) + Models --> output\n",
    "```\n",
    "- 수학 모델:\n",
    "  - Feature Representation : Spectrogram(O)\n",
    "  - Pronunciation Model : 어휘 모델(Q)\n",
    "  - Acoustic Model : 음향 모델(A)\n",
    "  - Language Model : 언어 모델(W)\n",
    "  - 의 Marginal Baysian!(P(W|O) = sum_A(P(W|A)P(A|Q)P(Q|O)))\n",
    "  - HMM\n",
    "- https://kaldi-asr.org/(Kaldi Speech Recognition Toolkit)\n",
    "\n",
    "- Feature Extraction\n",
    "  - Spectrogram(MFCC, Mel Filter bank or Power Spectrogram)\n",
    "  - WAV2Vec\n",
    "\n",
    "- Acoustic Model\n",
    "  - k-HMM Model\n",
    "  - DNN-HMM\n",
    "  - RNN-CTC(Connectionist Temporal Classification) : 실시간 인식\n",
    "  - End2End에서는 HMM이 더이상 사용되지 않는다\n",
    "  \n",
    "\n",
    "- Pronunciation Model\n",
    "  - Lexicon: 인식 단어(Word)를 발음열(Phone Seq)로 표현하여 상태를 정의\n",
    "  - 어휘 사전에 없는 단어는 인식될 수 없다\n",
    "  - 조사 등을 분리해서 저장해야 보다 다양한 어휘를 등록할 수 있다\n",
    "  - 신조어를 어떻게 처리하는지가 문제!(등재해야만 한다)\n",
    "  - End2End 음성 인식에서는 안쓰이지만, 음성 합성에서는 사용된다\n",
    " \n",
    "- Decoding\n",
    "  - Self loop : 말을 길게 늘이는 경우를 대처하기 위해 사용\n",
    "\n",
    "\n",
    "- Online(Streaming)\n",
    "  - 실시간 데이터 처리(gRPC)\n",
    "  \n",
    "### 최근의 음성 인식\n",
    "- Speech-Transformer\n",
    "  - Spectrogram을 만든 후, VGG를 돌린다\n",
    "    - Spectrogram을 이미지로 인식(but, 가변 이미지이다)\n",
    "  - 나머지는 Transformer!\n",
    "- Wave2Vec\n",
    "  - 양자화 : Clustring 하여 기준점 찾기\n",
    "  \n",
    "### 여러 모델들\n",
    "- Multispeaker Tacotron : 화자의 목소리를 바꾸기(A에서 B로)\n",
    "- Voice Conversion : 화자의 목소리를 바꾸기\n",
    "- typecast : 화자 바꾸기(맥락 고려 X)\n",
    "- 텍스트로 맥락을 이해하는건 어렵다\n",
    "- 오디오, 얼굴 영상 등이 있다면, 훨씬 쉬움!\n",
    "- Voice Filter: 겹쳐있는 음성을 분리\n",
    "https://www.scalenut.com/blogs/ai-writes-scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f17a2d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4b2f17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
