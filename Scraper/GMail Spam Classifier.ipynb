{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "210967a3",
   "metadata": {},
   "source": [
    "# Scraper\n",
    "```\n",
    "State := Current site, collected data, links, times, etc\n",
    "Init := (Inital site, 0, 0, 0, ...)\n",
    "Goal := (*, specific amount of data, *, timeout, ...)\n",
    "Action := Start | Collect | Travel | Stop | Exit\n",
    "    Start := Execute the scraper in S if it is valid\n",
    "    Collect := Collect useful data and links. Test whether it is new or not\n",
    "    Travel := Move S2 via links\n",
    "    Stop := Stop during a little time\n",
    "    Exit := Exit the scraper\n",
    "Transition :=\n",
    "    (Start, S1) -> (Collect, S1) | (Travel, S1, S2 in links) | (Stop, S1) | (Exit, S1)\n",
    "    (Collect, S1) -> (Travel, S1, S2 in links) | (Stop, S1) | (Exit, S1)\n",
    "    (Travel, S1, S2) -> (Start, S2)\n",
    "    (Stop, S1) -> (Collect, S1)\n",
    "    (Exit, S1) -> Done\n",
    "Action Cost := Equal\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "id": "c8ad1c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abstract Class\n",
    "import time\n",
    "import random\n",
    "from functools import wraps\n",
    "from operator import lt, eq\n",
    "from copy import copy, deepcopy\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from abc import ABCMeta, abstractmethod\n",
    "\n",
    "import requests\n",
    "from urllib.error import URLError\n",
    "from requests.exceptions import HTTPError\n",
    "from urllib.robotparser import RobotFileParser\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ScraperState:\n",
    "    def __init__(self, site: str, data: dict = dict(), links: set = set(), times:float = 0.):\n",
    "        self.site = site\n",
    "        self.data = data\n",
    "        self.links = links\n",
    "        self.times = times\n",
    "    \n",
    "    def __copy__(self):\n",
    "        cls = self.__class__\n",
    "        result = cls.__new__(cls)\n",
    "        result.__dict__.update(self.__dict__)\n",
    "        return result\n",
    "\n",
    "    def __deepcopy__(self, memo):\n",
    "        cls = self.__class__\n",
    "        result = cls.__new__(cls)\n",
    "        memo[id(self)] = result\n",
    "        for k, v in self.__dict__.items():\n",
    "            setattr(result, k, deepcopy(v, memo))\n",
    "        return result\n",
    "\n",
    "class ABCScraper(metaclass=ABCMeta):\n",
    "    def __init__(self, init:ScraperState, goal:ScraperState):\n",
    "        self.__init = init\n",
    "        self.__current = deepcopy(self.__init)\n",
    "        self.__goal = goal\n",
    "        self.lastStep = \"Init\"\n",
    "    \n",
    "    @property\n",
    "    def initState(self):\n",
    "        return self.__init\n",
    "    @property\n",
    "    def currentState(self):\n",
    "        return self.__current\n",
    "    @property\n",
    "    def goalState(self):\n",
    "        return self.__goal\n",
    "    \n",
    "    def __canFetch():\n",
    "        robots = dict()\n",
    "        \n",
    "        def canFetch(self, path:\"uri\", ua:\"user-agent\" = '*') -> bool:\n",
    "            url = urljoin(path, \"/robots.txt\")\n",
    "            \n",
    "            if url in robots.keys():\n",
    "                return robots[url]\n",
    "            \n",
    "            try:\n",
    "                robotParser =  RobotFileParser(url)\n",
    "                robotParser.read()\n",
    "                robots[url] = robotParser.can_fetch(ua, path)\n",
    "                return robots[url]\n",
    "            except HTTPError as e:\n",
    "                if e.status_code // 100 == 4:\n",
    "                    robots[url] = True # robots.txt doesn't exist. So, the access is permitted\n",
    "                else:\n",
    "                    robots[url] = False\n",
    "                return robots[url]\n",
    "            except URLError:\n",
    "                raise # Invalid URL Error\n",
    "\n",
    "        return canFetch\n",
    "    canFetch = __canFetch()\n",
    "    \n",
    "    def meet_unexpected_error(e:\"Error\"):\n",
    "        print(\"Unexpected Error\")\n",
    "        print(e)\n",
    "\n",
    "    # Run\n",
    "    def run(self):\n",
    "        print(\"Running Scraper\")\n",
    "        return self.start()\n",
    "    \n",
    "    # Action\n",
    "    def start(self):\n",
    "        # Vaild URL Test\n",
    "        url = self.currentState.site\n",
    "        try:\n",
    "            # Check robots.txt\n",
    "#             if self.canFetch(url):\n",
    "#                 #Stop randomly\n",
    "#                 return self.collect() if random.randint(1, 100) > 10 else self.stop()\n",
    "#             else:\n",
    "#                 print(\"Robots.txt doesn't permit a scraper\")\n",
    "            \n",
    "#             link = self.select_link()\n",
    "#             if link:\n",
    "#                 return self.travel(link)\n",
    "#             else:\n",
    "#                 self.lastStep = 'Start'\n",
    "#                 return self.exit()\n",
    "\n",
    "            # Check robots.txt but ignore it\n",
    "            if not self.canFetch(url):\n",
    "                print(\"Robots.txt doesn't permit a scraper\")\n",
    "            return self.collect() if random.randint(1, 100) > 10 else self.stop()\n",
    "        except URLError:\n",
    "            print(\"ERROR: URLError\")\n",
    "            return self.exit()\n",
    "        except Exception as e:\n",
    "            ABCScraper.meet_unexpected_error(e)\n",
    "            return self.exit()\n",
    "    \n",
    "    @abstractmethod\n",
    "    def select_link(self):\n",
    "        '''Return selected link or None'''\n",
    "        pass\n",
    "    \n",
    "    def collect(self):\n",
    "        try:\n",
    "            self.collect_by_custom()\n",
    "        except HTTPError as e:\n",
    "            if e.response.status_code // 100 == 5: #5xx\n",
    "                return self.stop()\n",
    "            print(\"Code\", e.response.status_code)\n",
    "            print(\"Reason\", e.response.reason)\n",
    "            print(\"Req Header\", e.request.headers)\n",
    "            return self.exit()\n",
    "        except Exception as e:\n",
    "            ABCScraper.meet_unexpected_error(e)\n",
    "            return self.exit()\n",
    "            \n",
    "        else:\n",
    "            if not self.is_exit():\n",
    "                link = self.select_link()\n",
    "                if link:\n",
    "                    return self.travel(link)\n",
    "            self.lastStep = 'Collect'\n",
    "            return self.exit()\n",
    "    \n",
    "    @abstractmethod\n",
    "    def collect_by_custom(self):\n",
    "        '''make collect algorithm'''\n",
    "        pass\n",
    "    \n",
    "    def travel(self, link):\n",
    "        self.currentState.site = link\n",
    "        return self.start()\n",
    "    \n",
    "    def stop(self):\n",
    "        time.sleep(random.randint(1, 10))\n",
    "        return self.collect()\n",
    "        \n",
    "    def exit(self):\n",
    "        print(\"Last step is\", self.lastStep)\n",
    "        print(\"Last site is\", self.currentState.site)\n",
    "        print(\"Collected data size is\", len(list(self.currentState.data)))\n",
    "        print(\"Remaining links is\", len(self.currentState.links))\n",
    "        print(\"Collect time is\", self.currentState.times, 'secs')\n",
    "        return self.currentState\n",
    "    \n",
    "    @abstractmethod\n",
    "    def is_exit(self):\n",
    "        '''make exit condition. True if exit'''\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "id": "1a468d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from requests.compat import urljoin\n",
    "from bs4 import BeautifulSoup\n",
    "from enum import Enum, unique, auto\n",
    "\n",
    "@unique\n",
    "class MailType(Enum):\n",
    "    SPAM = auto()\n",
    "    NORMAL = auto()\n",
    "    \n",
    "    def __str__(self):\n",
    "        mail_type = ''\n",
    "        match self:\n",
    "            case MailType.SPAM:\n",
    "                mail_type = 'SPAM'\n",
    "            case MailType.NORMAL:\n",
    "                mail_type = 'NORMAL'\n",
    "        return mail_type\n",
    "\n",
    "@dataclass\n",
    "class Data:\n",
    "    sender: str\n",
    "    date: datetime.date\n",
    "    title: str\n",
    "    content: str\n",
    "    mail_type: MailType\n",
    "        \n",
    "    def to_dict(self):\n",
    "        return {'sender': self.sender, 'date': self.date, 'title': self.title,\\\n",
    "                'content': self.content, 'mail_type': str(self.mail_type)}\n",
    "    \n",
    "\n",
    "# Scraper\n",
    "class GMailScraper(ABCScraper):\n",
    "    def __init__(self, init:ScraperState, goal:ScraperState, configPath:\"Safari config file path\" = \"mail.google.com.har\"):\n",
    "        '''\n",
    "        HTTP Gmail url: https://mail.google.com/mail/u/0/h/\n",
    "        How to get config file:\n",
    "            1. Go to Safari\n",
    "            2. Click network in development-tools\n",
    "            3. Click \"export\" at right-side\n",
    "        '''\n",
    "        super().__init__(init, goal)\n",
    "        url, headers = self.__parse_config(configPath)\n",
    "        if not url or not headers:\n",
    "            print(\"GMailScraper cannot be initialized\")\n",
    "            return None\n",
    "        \n",
    "        self.currentState.data = []\n",
    "        self.headers = headers\n",
    "        self.mode = MailType.NORMAL\n",
    "        \n",
    "        # Collect_links\n",
    "        self.currentState.site = url\n",
    "        self.currentState.links = []\n",
    "        res = requests.get(url, headers=self.headers)\n",
    "        self.__collect_links(res)\n",
    "        self.currentState.site = self.select_link()\n",
    "        \n",
    "    def __parse_config(self, configPath)->(\"url\", \"headers\"):\n",
    "        try:\n",
    "            fp = open(configPath, 'r')\n",
    "            config = json.load(fp)\n",
    "            fp.close()    \n",
    "        except FileNotFoundError:\n",
    "            print(\"Invalid configuration Path\")\n",
    "            return None, None\n",
    "        except OSError:\n",
    "            print(\"Invalid configuration Path\")\n",
    "            return None, None\n",
    "        except Exception as e:\n",
    "            Scraper.meet_unexpected_error(e)\n",
    "            return None, None\n",
    "            \n",
    "        else:\n",
    "            try:\n",
    "                req = config['log']['entries'][0]['request']\n",
    "                headers = {attr['name']:attr['value'] for attr in req['headers']}\n",
    "                return req['url'], headers\n",
    "            except KeyError:\n",
    "                print(\"Invalid configuration file\")\n",
    "                return None, None\n",
    "            except IndexError:\n",
    "                print(\"Invalid configuration file\")\n",
    "                return None, None\n",
    "            except Exception as e:\n",
    "                Scraper.meet_unexpected_error(e)\n",
    "                return None, None\n",
    "                \n",
    "        \n",
    "    def collect_by_custom(self):\n",
    "        try:\n",
    "            res = requests.get(self.currentState.site, headers=self.headers)\n",
    "            res.raise_for_status()\n",
    "        except HTTPError as e:\n",
    "            print(e.response.status_code)\n",
    "            print(e.response.reason)\n",
    "            print(e.request.headers)\n",
    "        except URLError as e:\n",
    "            print(\"URLError\")\n",
    "        except Exception as e:\n",
    "            Scraper.meet_unexpected_error(e)\n",
    "        else:\n",
    "            self.__collect_data(res)\n",
    "            if not self.currentState.links:\n",
    "                self.__collect_links(res)\n",
    "        return\n",
    "    \n",
    "    \n",
    "    def __collect_data(self, res):\n",
    "        dom = BeautifulSoup(res.text, 'lxml')\n",
    "        sender = dom.find('table', {'class': 'h'}).next_sibling.find('table').find('h3').text\n",
    "        date = dom.find('table', {'class': 'h'}).next_sibling.find('table').find('td').next_sibling.text\n",
    "        date = re.search(r'^(\\d+)년 (\\d+)월 (\\d+)일', date)\n",
    "        date = datetime.date(int(date.group(1)), int(date.group(2)), int(date.group(3)))\n",
    "        \n",
    "        title = dom.find('table', {'class': 'h'}).find('h2').text\n",
    "\n",
    "        content = dom.find('div', {'class': 'msg'}).text\n",
    "        content = re.sub(r'\\u200c+', r' ', content)\n",
    "        content = re.sub(r'\\xa0+', r' ', content)\n",
    "        content = re.sub(r'(\\s)+', r'\\1', content).strip()\n",
    "        \n",
    "        if '스팸함' in dom.find('a', {'class': 'searchPageLink'}).text:\n",
    "            mail_type = MailType.SPAM\n",
    "        else:\n",
    "            mail_type = MailType.NORMAL\n",
    "            \n",
    "        datum = Data(sender, date, title, content, mail_type)\n",
    "        self.currentState.data.append(datum)\n",
    "        \n",
    "    \n",
    "    def __collect_links(self, res):\n",
    "        '''\n",
    "        link := [next_main_page, mail_pages]\n",
    "        if mail_pages == empty: next main_page is selected & collect mail_pages\n",
    "        '''\n",
    "        \n",
    "        dom = BeautifulSoup(res.text, 'lxml')\n",
    "        spam_mailbox_link = dom.find('table', {'class': 'm'}).find(string='스팸함').parent['href']\n",
    "        spam_mailbox_link = urljoin(self.currentState.site, spam_mailbox_link)\n",
    "\n",
    "        links = dom.find('table', {'class': 'ft'}).previous_sibling.find_all('a', {'class':'searchPageLink'})\n",
    "        next_page_link = ''\n",
    "        for link in links:\n",
    "            if '이전' in link.get_text():\n",
    "                nxt_page_link = urljoin(self.currentState.site, link['href'])\n",
    "                break\n",
    "\n",
    "        mail_links = set(link.parent['href'] for link in dom.find_all('span', {'class': 'ts'}))\n",
    "        mail_links = set(map(lambda link: urljoin(self.currentState.site, link), mail_links))\n",
    "\n",
    "        self.currentState.links.append((spam_mailbox_link, next_page_link, mail_links))\n",
    "        \n",
    "    \n",
    "    def select_link(self):\n",
    "        '''\n",
    "        select mail_pages not main_page\n",
    "        '''\n",
    "        \n",
    "        if not self.currentState.links:\n",
    "            return None\n",
    "        \n",
    "        spam_mailbox_link, next_page_link, mail_links = self.currentState.links.pop()\n",
    "\n",
    "        # Record\n",
    "        print(len(mail_links), self.mode)\n",
    "        if len(mail_links) != 0:\n",
    "            # Scrape mail\n",
    "            mail_link = mail_links.pop()\n",
    "            self.currentState.links.append((spam_mailbox_link, next_page_link, mail_links))\n",
    "            return mail_link\n",
    "        elif next_page_link:\n",
    "            # Move next page & Collect mail_links\n",
    "            res = requests.get(next_page_link, headers=self.headers)\n",
    "            self.__collect_links(res)\n",
    "            return self.select_link()\n",
    "        else:\n",
    "            if self.mode == MailType.NORMAL:\n",
    "                # Move Spam mailbox & Collect mail_link\n",
    "                self.mode = MailType.SPAM\n",
    "                res = requests.get(spam_mailbox_link, headers=self.headers)\n",
    "                self.__collect_links(res)\n",
    "                return self.select_link()\n",
    "            else:\n",
    "                #Done\n",
    "                return None\n",
    "    \n",
    "    def is_exit(self):\n",
    "        return True if not self.currentState.links else False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b529f5",
   "metadata": {},
   "source": [
    "# DB\n",
    "\n",
    "```\n",
    "Relation Scheme(mail)\n",
    "\n",
    "Data := json format [{\"sender\": sender, \"date\": date, \"title\": title, \"content\": content, \"type\":type}, ...]\n",
    "Sender := CHAR(50)\n",
    "Date := Date\n",
    "Title := CHAR(100)\n",
    "Content := TEXT\n",
    "Type := BOOLEAN (TRUE if Spam else Normal)\n",
    "\n",
    "\n",
    "Query := SaveQuery | SearchQuery\n",
    "SaveQuery := (\"SAVE\", _)\n",
    "SearchQuery := (\"SEARCH\", CONDITIONS SQL SYNTAX like title=\"test\")\n",
    "```\n",
    "\n",
    "```\n",
    "State := (data, query)\n",
    "Init := (data, saveQuery) | (_, SearchQuery)\n",
    "Goal := (_, saveQuery) | (data, SearchQuery)\n",
    "\n",
    "Action := start | connect | create | save | search | exit\n",
    "    start := Start db jobs\n",
    "    connect := Make a connection to db\n",
    "    create := Create relation scheme\n",
    "    save := Save data in db\n",
    "    search := Execute searchQuery in db\n",
    "    exit := Exit the db jobs.\n",
    "    \n",
    "Transition\n",
    "    (start, S1) := (connect, S1) | (save, S1) | (search, S1) | (exit, S1)\n",
    "    (connect, S1) := (start, S1) | (exit, S1)\n",
    "    (save, S1) := (create, S1) | (exit, S1)\n",
    "    (create, S1) := (save, S1) | (exit, S1)\n",
    "    (search, S1) := (exit, S1)\n",
    "    (exit, S1) := Done\n",
    "    \n",
    "Action Cost := Manually defined\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "id": "5391c413",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from abc import ABCMeta, abstractmethod, abstractproperty\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum, auto, unique\n",
    "import datetime\n",
    "\n",
    "@unique\n",
    "class QueryType(Enum):\n",
    "    SAVE = auto()\n",
    "    SEARCH = auto()\n",
    "    \n",
    "    def __str__(self):\n",
    "        query_type = ''\n",
    "        match self:\n",
    "            case QueryType.SAVE:\n",
    "                query_type = 'SAVE'\n",
    "            case QueryType.SEARCH:\n",
    "                query_type = 'SEARCH'\n",
    "        return query_type\n",
    "\n",
    "@dataclass\n",
    "class DBState:\n",
    "    data: list()\n",
    "    query: tuple()\n",
    "        \n",
    "class ABCSingleton(metaclass=ABCMeta):\n",
    "    __instances = {}\n",
    "    def __call__(cls, *args, **kwargs):\n",
    "        if cls not in __instances.keys():\n",
    "            cls.__instances[cls] = super().__call__(*args, **kwargs)\n",
    "        return cls.__instances[cls]\n",
    "    \n",
    "    \n",
    "class ABCDataBase(ABCSingleton):\n",
    "    def __init__(self, init:DBState, db:\"DataBase\", table_name: \"Relation\"):\n",
    "        '''\n",
    "        Init := (data, saveQuery) | (_, SearchQuery)\n",
    "        '''\n",
    "        self.lastStep = \"Init\"\n",
    "        self.__state = init\n",
    "        self.db = db\n",
    "        self.table_name = table_name\n",
    "        \n",
    "        self.__conns = []\n",
    "        self.__conn = None\n",
    "    \n",
    "    @property\n",
    "    def state(self):\n",
    "        return self.__state\n",
    "    \n",
    "    @state.setter\n",
    "    def state(self, new):\n",
    "        self.__state = new\n",
    "    \n",
    "    @property\n",
    "    def conn(self):\n",
    "        '''Current connection'''\n",
    "        return self.__conn\n",
    "    @conn.setter\n",
    "    def conn(self, new):\n",
    "        self.__conn = new\n",
    "    \n",
    "    @property\n",
    "    def conns(self):\n",
    "        '''Connection pool'''\n",
    "        return self.__conns\n",
    "    \n",
    "    def run(self):\n",
    "        print(\"Run...\")\n",
    "        print(f\"DB: {self.db}\")\n",
    "        print(f\"Table: {self.table_name}\")\n",
    "        print(f\"Mode: {str(self.state.query[0])}\")\n",
    "        return self.start()\n",
    "    \n",
    "    def start(self):\n",
    "        self.lastStep = \"Start\"\n",
    "        \n",
    "        if not len(self.conns):\n",
    "            return self.connect()\n",
    "        self.conn = self.conns.pop()\n",
    "        \n",
    "        queryType = self.state.query[0]\n",
    "        match queryType:\n",
    "            case QueryType.SAVE:\n",
    "                return self.save()\n",
    "            case QueryType.SEARCH:\n",
    "                return self.search()\n",
    "            case _:\n",
    "                print(\"Unexpected querytype\")\n",
    "                return self.exit()\n",
    "    \n",
    "    @abstractmethod\n",
    "    def connect(self):\n",
    "        '''Make a connection to db & Save it in __conns'''\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def save(self):\n",
    "        '''Save data in relation'''\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def create(self):\n",
    "        '''Create relation Scheme'''\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def search(self):\n",
    "        '''Search query & Fill it data in status'''\n",
    "        pass\n",
    "        \n",
    "        \n",
    "    def exit(self):\n",
    "        print(\"Last step is\", self.lastStep)\n",
    "        return self.state\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "id": "36a4e85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "class SQLiteDB(ABCDataBase):\n",
    "    def connect(self):\n",
    "        self.lastStep = \"Connect\"\n",
    "        try:\n",
    "            conn = sqlite3.connect(self.db)\n",
    "            self.conns.append(conn)\n",
    "            return self.start()\n",
    "        except sqlite3.Error as e:\n",
    "            print(\"An error occured:\", e.args[0])\n",
    "            return self.exit()\n",
    "        \n",
    "    def save(self):\n",
    "        self.lastStep = \"Save\"\n",
    "        try:\n",
    "            with self.conn as conn:\n",
    "                cur = conn.cursor()\n",
    "                cur.execute('''\n",
    "                SELECT name FROM sqlite_schema\n",
    "                WHERE type = 'table' AND name NOT LIKE 'sqlite_%';\n",
    "                ''')\n",
    "                tables = cur.fetchall()\n",
    "                if not any([True if self.table_name in table else False for table in tables]):\n",
    "                    print(\"Enter create\")\n",
    "                    return self.create()\n",
    "                keys = self.state.data[0].to_dict().keys()\n",
    "                data = [datum.to_dict() for datum in self.state.data]\n",
    "                cur.executemany(f'''\n",
    "                    INSERT INTO {self.table_name}({','.join(keys)})\n",
    "                    VALUES(:{',:'.join(keys)});\n",
    "                ''', data)\n",
    "                conn.commit()\n",
    "        except sqlite3.Error as e:\n",
    "            print(\"An error occured:\", e.args[0])\n",
    "        \n",
    "        return self.exit()\n",
    "        \n",
    "    def create(self):\n",
    "        self.lastStep = \"Create\"\n",
    "        try:\n",
    "            with self.conn as conn:\n",
    "                cur = conn.cursor()\n",
    "                cur.execute(f'''\n",
    "                    CREATE TABLE {self.table_name}(\n",
    "                        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                        sender CHAR(100) NOT NULL,\n",
    "                        date DATE NOT NULL,\n",
    "                        title CHAR(200) NOT NULL,\n",
    "                        content TEXT,\n",
    "                        mail_type CHAR(10) NOT NULL\n",
    "                    );\n",
    "                    ''')\n",
    "                conn.commit()\n",
    "            return self.save()\n",
    "        except sqlite3.Error as e:\n",
    "            print(\"An error occured:\", e.args[0])\n",
    "            return self.exit()\n",
    "            \n",
    "    def search(self):\n",
    "        self.lastStep = \"Search\"\n",
    "        try:\n",
    "            with self.conn as conn:\n",
    "                cur = conn.cursor()\n",
    "                condition = self.state.query[1]\n",
    "                if condition:\n",
    "                    cur.execute(f'''SELECT * FROM {self.table_name} WHERE {condition}''')\n",
    "                else:\n",
    "                    cur.execute(f'''SELECT * FROM {self.table_name}''')\n",
    "                data = [Data(sender, date, title, content, mail_type) \\\n",
    "                        for _, sender, date, title, content, mail_type in cur.fetchall()]\n",
    "                self.state.data = data\n",
    "                \n",
    "        except sqlite3.Error as e:\n",
    "            print(\"An error occured:\", e.args[0])\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        finally:\n",
    "            return self.exit()\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "id": "1f315af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 NORMAL\n",
      "Running Scraper\n",
      "Robots.txt doesn't permit a scraper\n",
      "4 NORMAL\n",
      "Robots.txt doesn't permit a scraper\n",
      "3 NORMAL\n",
      "Robots.txt doesn't permit a scraper\n",
      "2 NORMAL\n",
      "Robots.txt doesn't permit a scraper\n",
      "1 NORMAL\n",
      "Robots.txt doesn't permit a scraper\n",
      "0 NORMAL\n",
      "1 SPAM\n",
      "Robots.txt doesn't permit a scraper\n",
      "0 SPAM\n",
      "Last step is Collect\n",
      "Last site is https://mail.google.com/mail/u/0/h/j4zmj06whgd2/?&th=181a70ad5e7a13af&v=c&s=m\n",
      "Collected data size is 6\n",
      "Remaining links is 0\n",
      "Collect time is 0.0 secs\n",
      "Run...\n",
      "DB: mails.db\n",
      "Table: mail\n",
      "Mode: SAVE\n",
      "Last step is Save\n",
      "Run...\n",
      "DB: mails.db\n",
      "Table: mail\n",
      "Mode: SEARCH\n",
      "Last step is Search\n",
      "DBState(data=[Data(sender='Google Payments', date='2022-06-23', title='Google Payments 사용자 정보가 변경되었습니다.', content='Google Payments 사용자 정보 변경 완료 알림\\n관리자(Google)가 귀하의 사용자 정보를 변경했습니다. 아래의 변경사항을 검토하시기 바랍니다.\\nPayments 프로필:\\n윤영로\\nPayments 프로필 ID:\\n1171-6274-4141\\n정보를 변경한 관리자:\\nGoogle\\n새 연락처 정보:\\n윤영로\\n새 권한:\\n관리\\n새 이메일 환경설정:\\n모든 결제 이메일\\n변경을 원하지 않을 경우 Payments 프로필 관리자에게 문의하세요.\\n고객센터\\n문의하기\\n결제 프로필 ID: 1171-6274-4141\\nGoogle LLC 1600 Amphitheatre Parkway, Mountain View, CA 94043\\n본 이메일은 Google 제품 또는 계정 관련 중요 변경사항을 알려 드리기 위해 발송되는 필수 서비스 공지입니다.', mail_type='NORMAL'), Data(sender='Google Play', date='2022-06-23', title='Google Play 주문 영수증(2022. 6. 23.)', content='감사합니다.\\nGoogle Play에서 Google Ireland Limited의 무료 체험판을 신청하셨습니다. 무료 체험판은 2022. 7. 23.에 종료됩니다. 취소하지 않으면 무료 체험판이 끝난 후 자동으로\\n(현재\\n매월 ₩10,450)의 정기 결제 요금이 청구됩니다. 언제든지 취소하실 수 있습니다. 정기 결제 관리\\n주문 번호: GPY.5487-2552-2863-44581\\n주문 날짜: 2022. 6. 23. 오전 8시 29분 30초 GMT+9 내 계정: x837420@gmail.com\\n상품\\n가격\\nYouTube Premium (YouTube)\\n₩0\\n자동 갱신되는 정기 결제\\n합계:\\n₩0\\n(VAT ₩0 포함)\\n결제 방법:\\n하나-5069\\n정기 결제를 신청하면 취소하기 전까지 Google에서 위에 명시된 정기 결제 요금을 제공된 결제 수단으로 자동 청구하도록 허락하게 됩니다. 취소 방법 알아보기 이 이메일을 잘 보관해 두세요.\\n문의사항이 있는 경우 Google Ireland Limited을(를) 방문하세요. 앱과 게임은 Google Play에서 각 개발자가 제공 및 판매하며, 개발자의 이름이 Google LLC인 경우를 제외하고 Google LLC는 Play에 등록된 상품 또는 관련 거래에 대해 책임을 지지 않습니다.\\nGoogle Play\\n어디서나 즐길 수 있는 엔터테인먼트 통합 관리 서비스. 자세히 알아보기 ›\\nGoogle Play 주문 내역을 확인하세요.\\nGoogle Play 환불 정책 및 서비스 약관을 확인하세요.\\n© 2022 Google | All Rights Reserved.\\nGoogle Ireland Limited, Gordon House, Barrow Street, Dublin 4, 아일랜드\\n본 이메일 주소는 발신 전용이므로 답장하지 마시기 바랍니다. 도움이 필요하면 Google Play 고객센터를 방문하세요.', mail_type='NORMAL'), Data(sender='The Google Account Team', date='2022-06-26', title='Xxxx님, Google 계정 설정을 확인하여 Mac 기기에서 다음 단계를 진행하세요', content='Xxxx님, 안녕하세요\\nMac 기기에서 Google에 로그인해 주셔서 감사합니다Google 계정 설정이 알맞게 구성되어 있는지 확인해 주세요\\n개인 정보 보호 진단 완료하기\\n단계별 안내를 통해 나에게 알맞은 개인 정보 보호 설정을 선택하세요\\n완료하기\\n보안 진단 완료하기\\n보안 진단에서 계정 보안을 강화하기 위한 맞춤 권장사항을 확인하세요\\n완료하기\\nGoogle 계정을 최대한 활용하세요\\nGoogle에서 맞춤 도움말, 뉴스, 추천 항목을 보내 드립니다.\\n예, 최신 소식을 받습니다\\n최근에 Mac 기기에서 Google 계정에 로그인했기 때문에 x837420@gmail.com 주소로 발송된 정보 제공용 이메일입니다. 기기에서 처음으로 계정에 로그인했을 때 기기의 Google 서비스를 설정하는 방법을 안내하는 이메일을 받고 싶지 않다면 수신 거부해 주세요.\\n© 2022 Google LLC 1600 Amphitheatre Parkway, Mountain View, CA 94043', mail_type='NORMAL'), Data(sender='Google 커뮤니티팀', date='2022-06-23', title='Xxxx님, 새 Google 계정 설정을 완료해 주세요', content='내 계정 기능 및 서비스를 확인해보세요.\\nXxxx님, Google 계정 이용을 시작해 보세요 새 Google 계정을 사용하면 Google 제품 및 앱, 서비스를 이용할 수 있습니다. 계정 이용과 관련한 유용한 도움말을 아래에서 확인해 보세요.\\nGoogle 계정을 최대한 활용하세요\\nGoogle에서 맞춤 도움말, 뉴스, 추천 항목을 보내 드립니다.\\n예, 최신 소식을 받습니다\\nHey Google\\n작업을 처리하는 데 도움을 받아 보세요.\\n지금 사용해 보기\\n지금 사용해 보기\\nGoogle Play\\n마음에 쏙 드는 앱과 게임을 찾아보세요.\\nGoogle Play로 이동하기\\n옵션이 나에게 맞는지 확인하기\\n개인정보 보호 및 보안 옵션을 검토하고 변경하여 Google 사용 환경을 개선하세요.\\n확인\\n내 휴대전화 찾기\\n내 기기 찾기로 휴대전화의 정확한 위치를 파악하고 기기를 안전하게 보호하세요.\\n필요한 정보 찾기 고객센터에서 새 Google 계정에 관해 자세히 알아보세요.\\n본 이메일은 발신 전용입니다. 새 계정과 관련해 궁금하신 점이 있다면 고객센터에서 필요한 정보를 대부분 찾으실 수 있습니다.\\nGoogle LLC1600 Amphitheatre Parkway,Mountain View, CA 94043\\nGoogle 계정을 만드셨기 때문에 발송된 이메일입니다.', mail_type='NORMAL'), Data(sender='YouTube', date='2022-06-23', title='Welcome to YouTube Premium!', content=\"Hi Xxxx,\\nWelcome to your YouTube Premium membership!\\nYour 1-month trial begins immediately.\\nYour payment method will be charged monthly once your trial ends.\\nYou can explore, manage, and cancel your membership any time by visiting YouTube account settings.\\nGet started\\nWelcome aboard!\\nThe YouTube team\\nOrder Date\\nJun 23, 2022\\nOrder Number\\nGPY.5487-2552-2863-44581\\nCancellations: You can cancel your YouTube Premium membership at anytime. If you cancel, you'll still have access to YouTube Premium until the end of your billing period. Refund policy\\nNeed help? Contact support or go to our Help Center. Please don't reply to this email.\\nHelp Center\\n• Email options\\nYour transaction was processed by Google Ireland Limited, which can be reached at:\\nGoogle Ireland Limited, Attn: YouTube Paid Services, Gordon House, Barrow Street, Dublin 4, Ireland\\nYou received this email to provide information and updates around your YouTube Premium product or account.\\n© 2022 Google LLC d/b/a YouTube, 901 Cherry Ave, San Bruno, CA 94066\\nPaid Service Terms of Service\", mail_type='NORMAL'), Data(sender='YouTube Premium', date='2022-06-28', title='축하합니다 | YouTube Premium 회원이 되셨습니다', content='광고 없는 YouTube와 YouTube Music\\nYouTube Premium에 오신 것을 환영합니다\\n광고 없는 YouTube와 YouTube Music을 즐기세요.\\n멤버십 혜택을 모두 알아보세요.\\n광고 없는 감상과 백그라운드 재생\\n광고 없이 동영상을 시청하고 다른 앱을 사용하면서 계속 들을 수 있습니다\\n오프라인 저장\\n좋아하는 동영상을 오프라인에서도 감상하세요.\\nYouTube Music Premium\\n새로운 음악 앱에서 맞춤 추천 플레이리스트, 공식 앨범 등을 즐기세요.\\nYouTube Music 앱을 다운로드하세요. 멤버십에 포함되어 있습니다.\\n본 이메일은 YouTube Premium 제품 또는 계정과 관련된 정보 및 업데이트를 알려드리기 위해 발송되었습니다.\\n© Google LLC d/b/a YouTube 901 Cherry AveSan Bruno, CA 94066', mail_type='SPAM')], query=(<QueryType.SEARCH: 2>, ''))\n"
     ]
    }
   ],
   "source": [
    "# Concat Scraper & DB\n",
    "\n",
    "scraper = GMailScraper(ScraperState(\"\"), ScraperState(\"\"), \"mail.google.com.har\")\n",
    "result = scraper.run()\n",
    "\n",
    "db = SQLiteDB(DBState(result.data, (QueryType.SAVE, _)), \"mails.db\", \"mail\")\n",
    "db.run()\n",
    "db.state = DBState(_, (QueryType.SEARCH, \"\"))\n",
    "result = db.run()\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
