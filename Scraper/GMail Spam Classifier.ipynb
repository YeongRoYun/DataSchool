{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "210967a3",
   "metadata": {},
   "source": [
    "# Scraper\n",
    "```\n",
    "State := Current site, collected data, links, times, etc\n",
    "Init := (Inital site, 0, 0, 0, ...)\n",
    "Goal := (*, specific amount of data, *, timeout, ...)\n",
    "Action := Start | Collect | Travel | Stop | Exit\n",
    "    Start := Execute the scraper in S if it is valid\n",
    "    Collect := Collect useful data and links. Test whether it is new or not\n",
    "    Travel := Move S2 via links\n",
    "    Stop := Stop during a little time\n",
    "    Exit := Exit the scraper\n",
    "Transition :=\n",
    "    (Start, S1) -> (Collect, S1) | (Travel, S1, S2 in links) | (Stop, S1) | (Exit, S1)\n",
    "    (Collect, S1) -> (Travel, S1, S2 in links) | (Stop, S1) | (Exit, S1)\n",
    "    (Travel, S1, S2) -> (Start, S2)\n",
    "    (Stop, S1) -> (Collect, S1)\n",
    "    (Exit, S1) -> Done\n",
    "Action Cost := Manually defined\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c8ad1c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abstract Class\n",
    "import time\n",
    "import random\n",
    "from functools import wraps\n",
    "from operator import lt, eq\n",
    "from copy import copy, deepcopy\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from abc import ABCMeta, abstractmethod\n",
    "\n",
    "import requests\n",
    "from urllib.error import URLError\n",
    "from requests.exceptions import HTTPError\n",
    "from urllib.robotparser import RobotFileParser\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class State:\n",
    "    site: str\n",
    "    data: dict\n",
    "    links: set\n",
    "    times: float #seconds\n",
    "    \n",
    "    def __init__(self, site: str, data: dict = dict(), links: set = set(), times:float = 0.):\n",
    "        self.site = site\n",
    "        self.data = data\n",
    "        self.links = links\n",
    "        self.times = times\n",
    "    \n",
    "    def __copy__(self):\n",
    "        cls = self.__class__\n",
    "        result = cls.__new__(cls)\n",
    "        result.__dict__.update(self.__dict__)\n",
    "        return result\n",
    "\n",
    "    def __deepcopy__(self, memo):\n",
    "        cls = self.__class__\n",
    "        result = cls.__new__(cls)\n",
    "        memo[id(self)] = result\n",
    "        for k, v in self.__dict__.items():\n",
    "            setattr(result, k, deepcopy(v, memo))\n",
    "        return result\n",
    "\n",
    "class Scraper(metaclass=ABCMeta):\n",
    "    def __init__(self, init:State, goal:State):\n",
    "        self.__init = init\n",
    "        self.__current = deepcopy(self.__init)\n",
    "        self.__goal = goal\n",
    "        self.lastStep = None\n",
    "    \n",
    "    @property\n",
    "    def initState(self):\n",
    "        return self.__init\n",
    "    @property\n",
    "    def currentState(self):\n",
    "        return self.__current\n",
    "    @property\n",
    "    def goalState(self):\n",
    "        return self.__goal\n",
    "    \n",
    "    def __canFetch():\n",
    "        robots = dict()\n",
    "        \n",
    "        def canFetch(self, path:\"uri\", ua:\"user-agent\" = '*') -> bool:\n",
    "            url = urljoin(path, \"/robots.txt\")\n",
    "            \n",
    "            if url in robots.keys():\n",
    "                return robots[url]\n",
    "            \n",
    "            try:\n",
    "                robotParser =  RobotFileParser(url)\n",
    "                robotParser.read()\n",
    "                robots[url] = robotParser.can_fetch(ua, path)\n",
    "                return robots[url]\n",
    "            except HTTPError as e:\n",
    "                if e.status_code // 100 == 4:\n",
    "                    robots[url] = True # robots.txt doesn't exist. So, the access is permitted\n",
    "                else:\n",
    "                    robots[url] = False\n",
    "                return robots[url]\n",
    "            except URLError:\n",
    "                raise # Invalid URL Error\n",
    "\n",
    "        return canFetch\n",
    "    canFetch = __canFetch()\n",
    "    \n",
    "    def __meet_unexpected_error(e:\"Error\"):\n",
    "        print(\"Unexpected Error\")\n",
    "        print(e)\n",
    "\n",
    "    # Run\n",
    "    def run(self):\n",
    "        print(\"Running Scraper\")\n",
    "        return self.start()\n",
    "    \n",
    "    # Action\n",
    "    def start(self):\n",
    "        # Vaild URL Test\n",
    "        url = self.currentState.site\n",
    "        try:\n",
    "            # Check robots.txt\n",
    "#             if self.canFetch(url):\n",
    "#                 #Stop randomly\n",
    "#                 return self.collect() if random.randint(1, 100) > 10 else self.stop()\n",
    "#             else:\n",
    "#                 print(\"Robots.txt doesn't permit a scraper\")\n",
    "            \n",
    "#             link = self.select_link()\n",
    "#             if link:\n",
    "#                 return self.travel(link)\n",
    "#             else:\n",
    "#                 self.lastStep = 'Start'\n",
    "#                 return self.exit()\n",
    "\n",
    "            # Check robots.txt but ignore it\n",
    "            if not self.canFetch(url):\n",
    "                print(\"Robots.txt doesn't permit a scraper\")\n",
    "            return self.collect() if random.randint(1, 100) > 10 else self.stop()\n",
    "        except URLError:\n",
    "            print(\"ERROR: URLError\")\n",
    "            return self.exit()\n",
    "        except Exception as e:\n",
    "            Scraper.__meet_unexpected_error(e)\n",
    "            return self.exit()\n",
    "    \n",
    "    @abstractmethod\n",
    "    def select_link(self):\n",
    "        '''Return selected link or None'''\n",
    "        pass\n",
    "    \n",
    "    def collect(self):\n",
    "        try:\n",
    "            self.collect_by_custom()\n",
    "        except HTTPError as e:\n",
    "            if e.response.status_code // 100 == 5: #5xx\n",
    "                return self.stop()\n",
    "            print(\"Code\", e.response.status_code)\n",
    "            print(\"Reason\", e.response.reason)\n",
    "            print(\"Req Header\", e.request.headers)\n",
    "            return self.exit()\n",
    "        except Exception as e:\n",
    "            Scraper.__meet_unexpected_error(e)\n",
    "            return self.exit()\n",
    "            \n",
    "        else:\n",
    "            if not self.is_exit():\n",
    "                link = self.select_link()\n",
    "                if link:\n",
    "                    return self.travel(link)\n",
    "            self.lastStep = 'Collect'\n",
    "            return self.exit()\n",
    "    \n",
    "    @abstractmethod\n",
    "    def collect_by_custom(self):\n",
    "        '''make collect algorithm'''\n",
    "        pass\n",
    "    \n",
    "    def travel(self, link):\n",
    "        self.currentState.site = link\n",
    "        return self.start()\n",
    "    \n",
    "    def stop(self):\n",
    "        time.sleep(random.randint(1, 10))\n",
    "        return self.collect()\n",
    "        \n",
    "    def exit(self):\n",
    "        print(\"Last step is\", self.lastStep)\n",
    "        print(\"Last site is\", self.currentState.site)\n",
    "        print(\"Collected data size is\", len(self.currentState.data))\n",
    "        print(\"Remaining links is\", len(set(self.currentState.links)))\n",
    "        print(\"Collect time is\", self.currentState.times, 'secs')\n",
    "        return self.currentState\n",
    "    \n",
    "    @abstractmethod\n",
    "    def is_exit(self):\n",
    "        '''make exit condition. True if exit'''\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1a468d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from collections import ChainMap\n",
    "from requests.compat import urljoin\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Scraper\n",
    "class GMailScraper(Scraper):\n",
    "    def __init__(self, init:State, goal:State, configPath:\"Safari config file path\" = \"mail.google.com.har\"):\n",
    "        '''\n",
    "        HTTP Gmail url: https://mail.google.com/mail/u/0/h/\n",
    "        How to get config file:\n",
    "            1. Go to Safari\n",
    "            2. Click network in development-tools\n",
    "            3. Click \"export\" at right-side\n",
    "        '''\n",
    "        super().__init__(init, goal)\n",
    "        url, headers = self.__parse_config(configPath)\n",
    "        if not url or not headers:\n",
    "            print(\"GMailScraper cannot be initialized\")\n",
    "            return None\n",
    "        self.currentState.site = url\n",
    "        self.headers = headers\n",
    "        \n",
    "    def __parse_config(self, configPath)->(\"url\", \"headers\"):\n",
    "        try:\n",
    "            fp = open(configPath, 'r')\n",
    "            config = json.load(fp)\n",
    "            fp.close()    \n",
    "        except FileNotFoundError:\n",
    "            print(\"Invalid configuration Path\")\n",
    "            return None, None\n",
    "        except OSError:\n",
    "            print(\"Invalid configuration Path\")\n",
    "            return None, None\n",
    "        except Exception as e:\n",
    "            super.__meet_unexpected_error(e)\n",
    "            return None, None\n",
    "            \n",
    "        else:\n",
    "            try:\n",
    "                req = config['log']['entries'][0]['request']\n",
    "                headers = {attr['name']:attr['value'] for attr in req['headers']}\n",
    "                return req['url'], headers\n",
    "            except KeyError:\n",
    "                print(\"Invalid configuration file\")\n",
    "                return None, None\n",
    "            except IndexError:\n",
    "                print(\"Invalid configuration file\")\n",
    "                return None, None\n",
    "            except Exception as e:\n",
    "                super.__meet_unexpected_error(e)\n",
    "                return None, None\n",
    "                \n",
    "        \n",
    "\n",
    "    def collect_by_custom(self):\n",
    "        try:\n",
    "            res = requests.get(self.currentState.site, headers=self.headers)\n",
    "            res.raise_for_status()\n",
    "        except HTTPError as e:\n",
    "            print(e.response.status_code)\n",
    "            print(e.response.reason)\n",
    "            print(e.request.headers)\n",
    "        except URLError as e:\n",
    "            print(\"URLError\")\n",
    "        except Exception as e:\n",
    "            super.__meet_unexpected_error(e)\n",
    "        else:\n",
    "            self.__collect_data(res)\n",
    "            self.__collect_links(res)\n",
    "        return\n",
    "    \n",
    "    def __collect_data(self, res):\n",
    "        dom = BeautifulSoup(res.text, 'lxml')\n",
    "        mails = dom.find('table', {'class': 'th'}).find_all('tr')\n",
    "        data = {mail.select('tr td:nth-of-type(2)')[0].get_text() : \\\n",
    "                    mail.select('tr td:nth-of-type(3)')[0].get_text() for mail in mails}\n",
    "\n",
    "        halfSpace = r'\\u200c'\n",
    "        for k, v in data.items():\n",
    "            newV = re.sub(halfSpace, ' ', v).strip()\n",
    "            data[k] = newV\n",
    "        self.currentState.data = ChainMap(self.currentState.data, data)\n",
    "        return\n",
    "    \n",
    "    def __collect_links(self, res):\n",
    "        dom = BeautifulSoup(res.text, 'lxml')\n",
    "        links = dom.find('table', {'class': 'ft'}).previous_sibling.find_all('a', {'class':'searchPageLink'})\n",
    "        \n",
    "        previous, recent, latest = None, None, None\n",
    "        for link in links:\n",
    "            # raw val = 이전   ›\n",
    "            form = re.sub(r'›', '', link.get_text()).strip()\n",
    "            match form:\n",
    "                case \"이전\":\n",
    "                    previous = link['href']\n",
    "                case \"다음\":\n",
    "                    recent = link['href']\n",
    "                case \"처음\":\n",
    "                    latest = link['href']\n",
    "                case _:\n",
    "                    continue\n",
    "        self.currentState.links.add((previous, recent, latest))\n",
    "        return\n",
    "        \n",
    "    \n",
    "    def select_link(self):\n",
    "        if len(self.currentState.links):\n",
    "            previous, _, _ = self.currentState.links.pop()\n",
    "            return urljoin(self.currentState.site, nxt)\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def is_exit(self):\n",
    "        return True if not len(self.currentState.links) else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c1e04094",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Scraper\n",
      "Robots.txt doesn't permit a scraper\n",
      "Unexpected Error\n",
      "name 'nxt' is not defined\n",
      "Last step is None\n",
      "Last site is https://mail.google.com/mail/u/0/h/2zlhprnm75py/?zy=c&view&f=1\n",
      "Collected data size is 6\n",
      "Remaining links is 0\n",
      "Collect time is 0.0 secs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "State(site='https://mail.google.com/mail/u/0/h/2zlhprnm75py/?zy=c&view&f=1', data=ChainMap({}, {'YouTube Premium': '축하합니다 | YouTube Premium 회원이 되셨습니다 - 광고 없는 YouTube와 YouTube Music YouTube Premium YouTube Premium에 오신 것을 환영합니다 광고 없는 YouTube와 YouTube Music을 즐기세요. 멤버십 혜택을 모두 알아보세요. 광고 없는 감상과 백그라운드 재생 광고 없이 동영상을 시청하고 다른 앱을 사용하면서 계속 들을 수 있습니다 오프라인 저장 좋아하는', 'The Google Account Team': 'Xxxx님, Google 계정 설정을 확인하여 Mac 기기에서 다음 단계를 진행하세요 - Xxxx님, 안녕하세요 Mac 기기에서 Google에 로그인해 주셔서 감사합니다 Google 계정 설정이 알맞게 구성되어 있는지 확인해 주세요 개인 정보 보호 진단 완료하기 단계별 안내를 통해 나에게 알맞은 개인 정보 보호 설정을 선택하세요 완료하기 보안 진단 완료하기 보안 진단에서 계정 보안을 강화하기 위한 맞춤 권장사항을 확인하세요 완료하기 Google', 'YouTube': 'Welcome to YouTube Premium! - Hi Xxxx, Welcome to your YouTube Premium membership! Your 1-month trial begins immediately. Your payment method will be charged monthly once your trial ends. You can explore, manage, and cancel your', 'Google Play': 'Google Play 주문 영수증(2022. 6. 23.) - Google Play 감사합니다. Google Play에서 Google Ireland Limited의 무료 체험판을 신청하셨습니다. 무료 체험판은 2022. 7. 23.에 종료됩니다. 취소하지 않으면 무료 체험판이 끝난 후 자동으로 (현재 매월 ₩10450)의 정기 결제 요금이 청구됩니다. 언제든지 취소하실 수 있습니다. 정기 결제 관리 주문 번호: GPY.', 'Google Payments': 'Google Payments 사용자 정보가 변경되었습니다. - Google Payments 사용자 정보 변경 완료 알림 관리자(Google)가 귀하의 사용자 정보를 변경했습니다. 아래의 변경사항을 검토하시기 바랍니다. Payments 프로필: 윤영로 Payments 프로필 ID: 1171-6274-4141 정보를 변경한 관리자: Google 새 연락처 정보: 윤영로 새 권한: 관리 새 이메일 환경설정: 모든 결제 이메일', 'Google 커뮤니티팀': 'Xxxx님, 새 Google 계정 설정을 완료해 주세요 - 내 계정 기능 및 서비스를 확인해보세요.'}), links=set(), times=0.0)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = GMailScraper(State(\"\"), State(\"\"), \"mail.google.com.har\")\n",
    "x.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b529f5",
   "metadata": {},
   "source": [
    "# DB\n",
    "```\n",
    "some structure\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5391c413",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
